{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Работаем с выгрзукой данных из базы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортиурем библиотеку для работы с БД на python\n",
    "import psycopg2\n",
    "\n",
    "# Пытаемся подключиться к БД, при ошибке выводим предупреждение\n",
    "try: \n",
    "    conn = psycopg2.connect(dbname='postgres', user='postgres', password='123', host='localhost')\n",
    "    # Получаем объект курсора, для запросов к БД\n",
    "    cursor = conn.cursor()\n",
    "except:\n",
    "    print('Неудалось подключиться к БД!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлекаем данные запросом из БД\n",
    "cursor.execute('SELECT name, description, sallary, region_name FROM jobschema.joblist')\n",
    "# Загружаем данные из запроса и преобрабатываем\n",
    "all_info = list(map(list, cursor.fetchall()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Извлекаем данные запросом из БД\n",
    "cursor.execute('SELECT skills FROM jobschema.joblist')\n",
    "# Загружаем данные из запроса и предобрабатываем\n",
    "all_skills = list(set(sum([i[0].split(';') for i in list(map(list, cursor.fetchall()))], [])))\n",
    "# Удаляем пустой элемент из списка\n",
    "all_skills.pop(0)\n",
    "# Выходим из курсора\n",
    "cursor.close()\n",
    "# Отключаемся от БД\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Предобрабатываем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортиурем библиотеку для леметизации предложений\n",
    "import spacy\n",
    "# Импортируем бибилиотеку для работы со строками\n",
    "import re\n",
    "\n",
    "# Загружаем преобученную модель для лемитизации русских слов\n",
    "nlp = spacy.load(\"ru_core_news_sm\")\n",
    "\n",
    "# Циклом проходимся по всем описаниям из выгруженных данных\n",
    "for i in range(len(all_info)):\n",
    "    # Удаляем из описания знаки препинания \n",
    "    description = re.sub(r'[.,?:!\"-()\\';{}]', r'', all_info[i][1])\n",
    "    # Иницилизируем объект nlp\n",
    "    doc = nlp(description)\n",
    "    # Заменяем прошлое описание из выгруженных данных на леметизированное описание\n",
    "    all_info[i][1] = \" \".join([token.lemma_ for token in doc])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Реализация NLP работы с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pupki\\anaconda3\\envs\\nlp\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uml -> Навык\n",
      "microsoft powerpoint -> Навык\n",
      "многозадачность -> Навык\n",
      "самоорганизованность -> Навык\n",
      "teradata -> Навык\n",
      "моделирование рисков -> Навык\n"
     ]
    }
   ],
   "source": [
    "# Импортиурем бибилиотеку для работы с нейросетями\n",
    "import torch\n",
    "# Импортиурем библиотеку для загрузки оттуда предобученного трасформера типа BERT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# Импортиурем из библиотеки реализацию алгоритма к-ближайших соседей\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Загрузка токенизатора\n",
    "tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "# Загрузка модели RuBERT\n",
    "model = BertModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "# Создание размеченного словаря, с помщью которого мы обучим нашу модель\n",
    "d = {i: \"Навык\" for i in all_skills} | {i: \"Не навык\" for i in [\"яблоко\", \"пенек\", \"машина\", \"дерево\", \"карандаш\", \"стул\", \"стол\", \"pencil\", \"pen\", \"car\", \"door\", \"tree\", \"window\", \"окно\"]}\n",
    "\n",
    "# Передаем X данные для обучения\n",
    "words = list(d.keys())\n",
    "\n",
    "# Массив для получение векторных представлений слов с помощью RuBERT\n",
    "word_embeddings = []\n",
    "# Циклом проходимся по всем переданным данным\n",
    "for word in words:\n",
    "    # Токенизация слов и преобразование в тензор\n",
    "    inputs = tokenizer(word, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        # Получение выходного состояния модели BERT\n",
    "        outputs = model(**inputs)\n",
    "        # Усреднение по токенам\n",
    "        word_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "        # Добавляем эмбеддинг в ранее созданый массив\n",
    "        word_embeddings.append(word_embedding)\n",
    "\n",
    "# Передаем в переменную импортированный ранее класс для реализации алгоритма K-NN\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=1) \n",
    "# Фитим нашу модель\n",
    "knn_classifier.fit(word_embeddings, list(d.values()))\n",
    "\n",
    "# Предиктим для проверки наши данные\n",
    "predicted_classes = knn_classifier.predict(word_embeddings)\n",
    "\n",
    "count = 0\n",
    "# Вывод результатов примера классификации слов\n",
    "for word, predicted_class in zip(words, predicted_classes):\n",
    "    print(word, \"->\", predicted_class)\n",
    "    if count == 5:\n",
    "        break\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# Импортиурем бибилиотеку для работы с нейросетями\n",
    "import torch\n",
    "# Импортиурем библиотеку для загрузки оттуда предобученного трасформера типа BERT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Загрузка токенизатора\n",
    "tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "# Загрузка модели RuBERT\n",
    "model = BertModel.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "# Вытаскивание описания из данных\n",
    "texts = [info[1] for info in all_info]\n",
    "\n",
    "# Создание списка для хранения предсказанных классов для каждого предложения\n",
    "predicted_classes_per_sentence = []\n",
    "\n",
    "# Циклом проходимся по всем переданным данным\n",
    "for text in texts:\n",
    "    # Из предложения делаем массив\n",
    "    words = text.split()\n",
    "    # Создаем массив для эмбеддинга слов\n",
    "    word_embeddings = []\n",
    "    # Проходимся по всем словам из предложения\n",
    "    for word in words:\n",
    "        # Токенизация слов и преобразование в тензор\n",
    "        inputs = tokenizer(word, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "        with torch.no_grad():\n",
    "            # Получение выходного состояния модели BERT\n",
    "            outputs = model(**inputs)\n",
    "            # Усреднение по токенам\n",
    "            word_embedding = outputs.last_hidden_state.mean(dim=1).squeeze().numpy()\n",
    "            # Добавляем полученный эмбеддинг в ранее созданный массив\n",
    "            word_embeddings.append(word_embedding)\n",
    "    \n",
    "    # Классификация слов текущего предложения\n",
    "    predicted_classes = knn_classifier.predict(word_embeddings)\n",
    "    # Добавляем в ранее созданный массив предсказанные классы\n",
    "    predicted_classes_per_sentence.append(predicted_classes)\n",
    "\n",
    "\n",
    "# Добавляем в каждый массив с данными еще одно поле, где будем хранить навыки из описания\n",
    "for i in range(len(all_info)):\n",
    "    all_info[i].append([])\n",
    "\n",
    "# Счетчик для перехода между данными вакансий\n",
    "count = 0\n",
    "# Проходимся по предложениям и их классам\n",
    "for sentence, predicted_classes in zip(texts, predicted_classes_per_sentence):\n",
    "    # Из предложения создаем массив со словами\n",
    "    sentence_words = sentence.split()\n",
    "    # Проходимся по каждому слову и проверяем его класс\n",
    "    for word, predicted_class in zip(sentence_words, predicted_classes):\n",
    "        # Если предсказанный класс = \"Навык\", то добавляем в новый созданный массив\n",
    "        if predicted_class == \"Навык\" and len(word) > 2:\n",
    "            all_info[count][4].append(word)\n",
    "    # Удаляем повторяющиеся навыки из массива \n",
    "    all_info[count][4] = list(set(all_info[count][4]))\n",
    "    # Увеличиваем счетчик\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pupki\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "c:\\Users\\pupki\\anaconda3\\envs\\nlp\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Импортурием из библиотеки scikit-learn модуль для работы с кластерами\n",
    "from sklearn.cluster import KMeans\n",
    "# Из бибилиотеки transformers загружаем предобученные модели на основе BERT\n",
    "from transformers import BertTokenizer, BertModel\n",
    "# Загружаем бибилиотеку для работы с нейросетями\n",
    "import torch\n",
    "\n",
    "# Загрузка токенизатора\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# Загрузка предобученной модели BERT\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Создаем массив для соединения всех навыком в одном месте\n",
    "all_detected_skills = []\n",
    "# Проходимся по всем данным\n",
    "for i in range(len(all_info)):\n",
    "    # Добавляем в ранее созданный массив навыки\n",
    "    all_detected_skills += all_info[i][4]\n",
    "\n",
    "# Максимальная длина последовательности\n",
    "max_length = 32\n",
    "\n",
    "# Векторизация текстов с учетом максимальной длины\n",
    "encoded_texts = [tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=max_length) for text in all_detected_skills]\n",
    "input_ids = torch.cat([encoded_text['input_ids'] for encoded_text in encoded_texts], dim=0)\n",
    "attention_masks = torch.cat([encoded_text['attention_mask'] for encoded_text in encoded_texts], dim=0)\n",
    "\n",
    "# Получение эмбеддингов текстов с помощью BERT\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_masks)\n",
    "    embeddings = outputs.last_hidden_state[:, 0, :].numpy()\n",
    "\n",
    "# Количество кластеров\n",
    "num_clusters = 3  \n",
    "# Добавляем модуль Kmeans для кластеризации\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "# Фитим добавленный алгоритм\n",
    "kmeans.fit(embeddings)\n",
    "\n",
    "# Создание массивов для навыков каждого кластера\n",
    "cluster_skills = [[] for _ in range(num_clusters)]\n",
    "\n",
    "# Заполнение массивов навыками, соответствующими кластерам\n",
    "for skill, cluster_label in zip(all_detected_skills, kmeans.labels_):\n",
    "    cluster_skills[cluster_label].append(skill)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Анализ полученных данных путем NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  - #### 4.1 Определение наиболее востребованных и наименее востребованных навыков для выбранной группы вакансий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее востребованные навыки: [('анализ', 9), ('аналитик', 9), ('данных', 9), ('sql', 9), ('знание', 8), ('разработка', 8), ('опыт', 8), ('бизнес', 7), ('данными', 7), ('python', 7)]\n",
      "\n",
      "Наиболее востребованные навыки: [('talks', 1), ('tages', 1), ('кастомных', 1), ('конкурентный', 1), ('дротчеты', 1), ('собеседование', 1), ('meetups', 1), ('tagesteam', 1), ('инжиниринговый', 1), ('странахс', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Импортиурем из бибилиотеки модуль для счетчика\n",
    "from collections import Counter\n",
    "\n",
    "# Считаем количество вхождения всех навыков\n",
    "skill_counts = Counter(all_detected_skills)\n",
    "\n",
    "# Выводим наиболее востребованные навыки\n",
    "print(f'Наиболее востребованные навыки: {skill_counts.most_common(10)}', end='\\n\\n')\n",
    "# Выводим наименее востребованные навыки\n",
    "print(f'Наиболее востребованные навыки: {skill_counts.most_common()[-10:]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - ##### 4.2 Определение наиболее высокооплачеваемых навыков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Наиболее олпачеваемые навыки: ['поставщик', 'powerbi', 'xml', 'знание', 'стартап', 'это', 'подарки', 'разработка', 'человекоориентированную', 'mysql', 'овертаймы', 'интегрировать', 'анализ', 'аналитик', 'вас', 'мвидео', 'qlikview', 'коммуникабельность', 'развивающийся', 'леруа', 'финансовый', 'быть', 'технониколь', 'аналитический', 'erp', 'неочевидный', 'tableu', 'усидчивость', 'бизнес', 'quotтехническийquot', 'аккредитованных', 'git', 'sheet', 'мдф', 'визуализация', 'как', 'tagesteam', 'фитнесс', 'собеседование', 'покупатель', 'данными', 'пожелание', 'производство', 'meetups', 'структурированный', 'очистка', 'data', 'лдсп', 'стрессоустойчивость', 'чтобы', 'дэшбордов', 'автоматизация', 'овертайм', 'зао', 'оптовый', 'полис', 'оформиться', 'аккредитация', 'аналитика', 'документооборот', 'квалификация', 'активно', '0800', 'прогнозирование', 'продажа', 'что', 'отличный', 'рост', 'science', 'администрирование', 'ему', 'представить', 'также', 'api', 'маркетплейсов', 'продавец', 'owner', 'superset', 'поэтому', 'userelationship', 'свежесваренный', 'индексация', 'talks', 'хдф', 'банк', 'производствоспециализируемся', 'фурнитура', 'доплата', 'результат', 'sql', 'excel', 'кастомных', 'switch', 'temporary', 'онлайн', 'подрядчик', 'http', 'случаться', 'софинансирование', 'маркетплейсах', 'заработный', 'себя', 'кофеваркой', 'таблицами', 'dash', 'согласно', 'word', 'существовать', 'отпуск', 'apache', 'javascript', 'дашборды', 'формулами', 'поставка', 'консультация', 'trino', 'выстраивание', 'производственные•', 'отрасль', 'дилер', 'фанеру', 'комплектующих', 'формализация', 'релокации', 'циан', 'дашбордов', 'даже', 'dax', 'доработка', 'plotly', 'product', 'отсрочка', 'tages', '5/2', 'saas', 'etl', 'get', 'чфмк', 'visiology', 'холдинг', 'конкурентный', 'публикация', 'google', 'праздники', 'athena', 'покто', 'конкурс', 'engineering', 'реляционный', 'дротчеты', 'ведомственный', 'действительно', 'иностранные•', 'ижевск', 'quotбелаяquot', 'странахс', 'гугл', 'tables', 'ux-', 'субсидирование', 'вложенные', 'преимущество', 'json', 'dwh', 'агрегировать', 'внятный', 'дома', 'поддержка', 'данных', 'чай', 'битрикс', 'заказчик', 'presto', 'python', 'проектирование', 'производите', 'трудоустройство', 'внимательность', 'данные', 'проактивность•', 'математический', 'обучаемость', 'программирование', 'clickhouse', 'бесплатный', 'саморазвитие', 'опыт', 'рынок', 'зачем', 'окупались', 'экспертиза', 'так', 'обучение', 'форматами', 'тебе', 'директор', 'твой', 'систематический', 'инжиниринговый', 'отчётность', 'выручка', 'ексель', 'нам', 'зарегистрировать', 'розничный']\n"
     ]
    }
   ],
   "source": [
    "# Создаем массив, куда сохраним наиболее высокооплачеваемые навыки\n",
    "most_paid_skills = []\n",
    "# Проходимся по всем данным\n",
    "for i in range(len(all_info)):\n",
    "    # Проверяем условие, если запрплата больше средней зарплаты всех вакансий, то добавляем в массив навыки\n",
    "    if all_info[i][2] > 28906:\n",
    "        most_paid_skills += all_info[i][4]\n",
    "# Удаляем дубликаты навыков из массива\n",
    "most_paid_skills = list(set(most_paid_skills))\n",
    "\n",
    "# Выводим полученные результаты\n",
    "print(f'Наиболее олпачеваемые навыки: {most_paid_skills}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   - ##### 4.3 Определение региональной специфики востребованных навыков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Новосибирская область: [('очистка', 1), ('tables', 1), ('xml', 1)]\n",
      "Москва: [('аналитик', 5), ('данных', 5), ('python', 5)]\n",
      "Нижегородская область: [('лдсп', 1), ('знание', 1), ('дэшбордов', 1)]\n",
      "Свердловская область: [('знание', 2), ('разработка', 2), ('анализ', 2)]\n",
      "Санкт-петербург: [('заработный', 1), ('себя', 1), ('кофеваркой', 1)]\n",
      "Воронежская область: []\n",
      "Брянская область: []\n",
      "Республика татарстан: []\n",
      "Московская область: []\n",
      "Алтайский край: []\n"
     ]
    }
   ],
   "source": [
    "# Импортиурем модуль для счетчика\n",
    "from collections import Counter\n",
    "\n",
    "# Создаем массив, куда сохраним наиболее востребованный навыки по регионам\n",
    "most_needed_skills = {name: sum([i[4] for i in all_info if i[3]==name], []) for name in [i[3] for i in all_info]}\n",
    "# Проходимся по полученному словарю\n",
    "for region, skills in most_needed_skills.items():\n",
    "    # Считаем количество вхождения навыка для региона\n",
    "    skill_counts = Counter(skills)\n",
    "    # Сохраняем навык с его счетчиком для региона\n",
    "    most_needed_skills[region] = skill_counts.most_common(3)\n",
    "\n",
    "# Выводим полученные результаты \n",
    "for key in most_needed_skills.keys():\n",
    "    print(f'{key.capitalize()}: {most_needed_skills[key]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Передача данных в Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импортиурем библиотеку для работы с Excel на питоне\n",
    "import openpyxl \n",
    "# Из бибилиотеки импортируем модуль счетчика\n",
    "from collections import Counter\n",
    "\n",
    "# Создаем массив для сохранения необходимых данных\n",
    "data_for_excel = [[] for i in cluster_skills]\n",
    "# Проходимся по всем кластерам\n",
    "for i in range(len(cluster_skills)):\n",
    "    # Считаем кол-во вхождений для каждого навыка в кластере\n",
    "    skill_counter = Counter(cluster_skills[i])\n",
    "    # Сохраняем в ранее созданный массив\n",
    "    data_for_excel[i] = skill_counter\n",
    "\n",
    "# Создание нового файла Excel\n",
    "wb = openpyxl.Workbook()\n",
    "# Получение активного листа\n",
    "sheet = wb.active\n",
    "\n",
    "# Заголовки столбцов\n",
    "headers = ['Навык', 'Количество', 'Кластер']\n",
    "\n",
    "# Запись заголовков в первую строку\n",
    "for col, header in enumerate(headers, start=1):\n",
    "    # Используем внутреннюю функцию бибилиотеки для записи названий стобцов\n",
    "    sheet.cell(row=1, column=col, value=header)\n",
    "\n",
    "# Стартовая строка для записи данных\n",
    "row_index = 2\n",
    "\n",
    "# Запись данных в таблицу для каждого кластера\n",
    "for cluster_index, cluster in enumerate(data_for_excel, start=1):\n",
    "    # Добавление заголовка для кластера\n",
    "    sheet.cell(row=row_index, column=3, value=f'Кластер {cluster_index}')\n",
    "    # Прибавляем к позиции строки +1\n",
    "    row_index += 1\n",
    "    # Запись данных из счетчика в таблицу\n",
    "    for skill, count in cluster.items():\n",
    "        sheet.cell(row=row_index, column=1, value=skill)\n",
    "        sheet.cell(row=row_index, column=2, value=count)\n",
    "        sheet.cell(row=row_index, column=3, value=f'Кластер {cluster_index}')  \n",
    "        row_index += 1\n",
    "\n",
    "# Сохранение файла Excel\n",
    "wb.save('mainskills.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____________________________________________________________________________________________________________________________________________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Сохраняю данные про навыки для следующего модуля, для построения графа"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш словарь данных с кортежами и их значениями\n",
    "data = [i[4] for i in all_info]\n",
    "\n",
    "# Открыть файл для записи\n",
    "with open('dataModuleB.txt', 'w') as f:\n",
    "    # Записать каждую пару кортеж и его значение в файл\n",
    "    for value in data:\n",
    "        f.write(f\"{value},\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
